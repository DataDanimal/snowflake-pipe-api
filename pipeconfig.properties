# Snowflake user
SNFLK_USER=<CHANGE_ME>
# Snowflake warehouse
SNFLK_WH=<CHANGE_ME>
# Snowflake database
SNFLK_DB=<CHANGE_ME>
# If you account has region embedded, then specify <account_id>.<region_id>
SNFLK_ACCT=<CHANGE_ME>
# Snowflake Schema
SNFLK_SCHEMA=pipe_api_stg
# Snowflake Pipe
SNFLK_PIPE=pipe_weather_daily_16_api_tsv_in
# Snowflake Role
SNFLK_ROLE=sysadmin
# Snowflake Stage name
SNFLK_STAGE=ext_stg_ingest
# Auto commit
SNFLK_AUTO_COMMIT=false
#SNFLK_TRACING=all
#  WARNING - Debug mode will print all DB properties to the console INCLUDING your password
#DEBUG_MODE=true
# DROP Objects to initialize the Pipe API results capture post API calls
DROP_OBJECTS=FALSE
# Log results will capture output at end as JSON for later analysis
LOG_RESULTS=TRUE
# Reset loging will clear prior results
RESET_LOGS=FALSE
# Public and Private key files for authentication
# Consider moving these to KMS
PRIVATE_KEY_FILE=/Users/<YOUR_NAME>/rsa_key.p8
# Private key passphrase
PRIVATE_KEY_PASSPHRASE=<PUT_HERE_AND_KEEP_SAFE>
# File extension to ingest, e.g. .csv.gz or .json.gz.  Case insensitive
PIPE_FILE_EXT=.CSV.GZ
# Show what happens on error, picks up .DS_Store
#PIPE_FILE_EXT=*.*
#Local path to source files.  This file must contain the following subfolders: pending, ready, complete, error,
PIPE_SOURCE_PATH=/Users/<YOUR_NAME>/pipe_api/weather
# AWS S3 Bucket name to files in S3 relative to external stage
S3_BUCKET=<YOUR_BUCKET_NAME_HERE>
# AWS region
AWS_REGION=us-west-2
# Path to files in S3 relative to external stage
PIPE_BASE_PATH=demo/inbound/weather
# The path relative to stage path where local files will be uploaded
PIPE_TARGET_PATH=test
// The batch window in milliseconds
BATCH_WINDOW=900000
# AWS S3 Access ID and secret
AWS_ACCESS_KEY_ID=<PUT_HERE_AND_KEEP_SAFE>
AWS_SECRET_ACCESS_KEY=<PUT_HERE_AND_KEEP_SAFE>